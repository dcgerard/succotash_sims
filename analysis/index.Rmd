---
title: "Home"
bibliography: sva_bib.bib
---

**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

## Known: Grid, Confounders, and Variances.

*  [Grid size of 3 with variances close together, $\pi_0$ overestimated.](alpha_sig_known.pdf)
*  [Grid size of 3 with one variance very far apart, $\pi_0$ accurately estimated.](alpha_sig_tau_known.pdf)


## Known: Grid and Confounders.

* [Variances drawn from $\chi^2_1$.](alpha_tau_known_1df_writeup.pdf)
* [Variances drawn from $\chi^2_5$.](alpha_tau_known_writeup.pdf)

## All Null Sims. Grid, Confounders, and Variance *unknown*.

* [Idealized data from SUCCOTASH model. SUCCOTASH underestimates lfdr but does better in terms of squared error loss.](count_theo_sims.pdf)
* [Same as above but using counts-per-million as output.](count_theo_sims_cpm.pdf)
* [Lung data, covariates randomized, n = 200, SUCCOTASH does best in terms of SSE and OK in terms of lfdr.](suc_v_rest_real_writeup.pdf)
* [Same as previous but with n = 40. Still best in terms of SSE but lfdr performance is horrible.](suc_v_rest_real_writeu_20v20.pdf)
* [RUV4's t-statistics means and standard deviations when and when not estimating a variance inflation parameter.](null_behavior.pdf)
* [Same as above but using t-likelihood when estimating the variance inflation parameter.](null_behaviort.pdf)

## Nonnull Sims. Grid, Confounders, and Variance *unknown*.

* [Idealized data from SUCCOTASH model. SUCCOTASH underestimates $\pi_0$ but does well in terms of squared error loss.](count_theo_nonnull_cpm.pdf)
* [31 competitors, varying n, $\pi_0$, and log2fold sd. SUCCOTASH does well for AUC and worst for estimating $\pi_0$.](varyNsampNullpiLog2sd.pdf)
* [Same as previous but with smaller $n \in \{3,5\}$. SUCCOTASH performs worst in terms of $\pi_0$.](varyNsampNullpiLog2sd_smallN.pdf)
* [(*)Explore different options for CATE. When $n = 20$, CATE does very well estimating $\pi_0$.](varyNsampNullpiLog2sd_cateonly.pdf)
* [Same as previous but with larger number of genes ($p = 10,000$). $n = 20$ is still pretty good.](varyNsampNullpiLog2sd_cateonly_10000.pdf)
* [Include SUCCOTASH (MOUTHWASH) into the CATE comparisons with $p = 1000$. Competitive in terms of AUC but horrible in terms of $\pi_0$.](varyNsampNullpiLog2sd_cateVsMouthwash.pdf)
* [Same as previous but include different regularization parameters in SUCCOTASH. Even *very* large values of $\lambda$ aren't enough.](calibrate_lambda.pdf)
* [FLASH + SUCCOTASH. Good at estimating $\pi_0$. Worse AUC than when using PCA.](varyNsampNullpiLog2sd_cateVsFlashMouthwash.pdf)
* [Homoscedastic variance factor analyses are better calibrated than heteroscedastic models](varyNsampNullpiLog2sd_cateVsMouthwashManyFA.pdf)
* [Empirically Bayesian shrinkage of the column-wise mean squared errors works better than using the raw column-wise mean squared errors, but worse than assuming homoscedasticity.](varyNsampNullpiLog2sd_cateVsMouthwashManyFA_shrinkPCA.pdf)
* [Initial sims using uniform mixtures. Variance model more important than mixing distribution. Homoscedasticity still works best.](unif_mix_compare.pdf)
* [SUCCOTASH vs ASH with homoscedastic and heteroscedastic assumptions and n = 10,000. Homoscedastic models are better calibrated but have worse AUC. SUCCOTASH generally has higher AUC](big_sims_mouth_largen_writeup.html)
* [Same as above but also including methods using limma-shrunk column-wise mean square as variance estimates.](big_sims_mouth_ashVsucc_writeup.html)
* [Correction of (*) where I do not inflate the variance. CATE performs much poorer in terms of estimating $\pi_0$](cate_redo.pdf)
* [Inflating variance results in improved estimates of $\pi_0$ and improved MSE](inflate_var_sims.pdf)
* [Estimating the scale of the variance works pretty well. Best at $\pi_0 = 0.5$ and competitively at $\pi_0 = 0.9$ and $1$](scale_var_sims.pdf)
* [No big improvements when use different factor analysis in scaled-variance SUCCOTASH](scale_var_sims_fa.pdf)
* [SVA + OLS + ASH while estimating variance inflation parameter does not work well in terms of estimating the null proportion.](sva_ash_varinflate.pdf) Also looked at using a penalty on the variance inflation parameter in SUCCOTASH, but only provided small improvement.s
* [Same as above but using CATE instead of SVA. Performance of ASH + CATE is same as ASH + SVA](ashr_cate.pdf)
* [Best results I've seen. Used a two step procedure of estimating, then inflating, variance scaling parameter for SUCCOTASH.](nopen_then_inflate.pdf)
* [Plots for same scenario as above, but included t-SUCCOTASH](muscle_tissue.pdf)
* [Plots for same scenario as above, but included RUV4 then ASH with t-likelihood](muscle_tissue_ruv_asht.pdf)
* [Using same alternatives as in ASH paper, SUCCOTASH has superior performance](diff_alttypes_muscle.pdf)
* [Same as above but including OLS + ASH. ASH does not perform well.](diff_alttypes_muscle_plus_ASH.pdf)
* [Same as above but use RUV + estimate scale using controls + ASH instead of ASH.](diff_alttypes_muscle_plus_ASH_RUV.pdf) RUV + ASH does great with variance inflation. Call this RUVASH.
* [Same as above but also limma-shrunk the variances prior to estimating scale and running ASH](diff_alttypes_muscle_plus_ASH_RUV_limma.pdf)
* [Similar to above but use bicrossvalidation to choose the number of hidden confounders. No big change in performance.](diff_alttypes_muscle_bcv.pdf)
* [pi0 vs pi0hat for various estimates.](pi0hat_sims.pdf) Akin to Figure 2b in @stephens2016false, but using the GTEX data plus Poisson thinning to generate signal. RUVASH and SUCCOTASH do the best.
* [Scale estimates in RUVASH invariant to alternative type.](scale_plot.pdf) Shrinks with sample size.
* [Using t-likelihood had very little effect on the performance of RUVASH compared to using the normal likelihood.](diff_alttypes_ruvasht.pdf)
* [Simulations using data generated from assumed model.](diff_alttypes_gaussian.pdf) Results are less clear. RUVASH still appears to perform best. Need to think about settings more carefully.
* [Same as above but using limma-shrunk variances in RUVASH and reduced variances of non-signal.](diff_alttypes_gaussian_limmashrink.pdf) RUVASH does great in terms of AUC, wins in estimating $\pi_0$ when $n$ is large.
* [Same as above, but used MAD-inflated + ASH methods.](diff_alttypes_muscle_mad.pdf) RUVASH still works the best. SUCCOTASH works the best among methods that don't use control genes.
* [Mad inflated + ASH methods using t-likelihood for MAD inflation and ASH.](muscle_tissue_madt.pdf) Same simulation settings as [here.](nopen_then_inflate.pdf)
* [Using SUCCOTASH to estimate unimodal inflation using control genes works ok.](muscle_tissue_succ_then_ash.pdf) It is slightly anticonservative for small n. For larger n, SUCCOTASH estimates the proportion of nulls to be near 1. In this case, the confounders are estimated the exact same as in GLS, and SUCCOTASH reduces to RUVASH. Don't know if there is any benefit to using unimodal inflation.
* [Simulations using n = 200.](muscle_tissue_largen.pdf) MAD inflated RUV and RUVASH work equally well for large $\pi_0$.
* [RUVASH simulations using alpha = 1](muscle_tissue_alpha1.pdf) This is where the t-statistics are assumed to be exchangeable. The coding is the first T/F is for posthoc_inflate the second T/F is for limmashrink. $\pi_0$ is still accurately estimated, but the post-hoc inflation seems to be less necessary. AUC is almost exactly the same when limmashrink = FALSE as cate_nc, which it should be. limmashrink = TRUE increases the AUC a little bit.
* [Additive inflation almost always estimated near zero.](muscle_tissue_adinf.pdf) Same coding as above. This is particularly true for n >= 10 and for limmashrunk variances.
* [Small improvement to RUVASH by using X AND Zhat to estimate standard errors of betahat.](different_ruvashes.pdf)
* [RUVASH using maximum number of factors allowed.](max_numsv.pdf) As long as limmashrunk variances are used, this actually works quite well.
* [Bias adjustment seems to slightly improve AUC but at the expense of slightl worse calibration.](adjust_bias.html)

## Other Implementation Checks.

* [Gives same results as `ashr` when no confounders.](check_with_ash.pdf)
* [An example where SUCCOTASH performs poorly](succotash_ests.html)
* [Grid is known, SUCCOTASH performs well](known_grid.html)
* [t-likelihood seems to be implemented correctly](test_tlike.pdf)
* [Scaling variances seems to be coded up correctly](scale_check.pdf)
* [Scaling factor probably not because of augmented design matrix.](explore_ruvmult.html) ASH lfdr ordering different when scaling variance, even though t-statistics remain unchanged.
* [Limma is scale-equivariant](limmashrink_invariance.html)
* [SUCCOTASH gives reasonable results no matter the starting positions.](muscle_succ_diff_init.pdf)
* [Same as above but one-step SUCCOTASH rather than two-step SUCCOTASH.](muscle_succ_diff_init_onestep.pdf)

## GTEX Analysis
* [Procedure for cleaning the GTEX data for heart.](gtex_clean.html)
* [Exact same as above but for muscle.](gtex_clean_muscle.html)
* [Exact same as above but for blood.](gtex_clean_blood.html)
* [RUV4 and RUVASH give very similar results for top genes in heart data.](analyze_cleaned_gtex.html)
* [Same plots as above but for muscle.](analyze_cleaned_gtex_muscle.html)
* [Same plots as above but for blood.](analyze_cleaned_gtex_blood.html)
* [Seeing variance inflation estimated differently in GTEX data vs all-null setting](inflation_vs_nsamp.html)
* [Used t-likelihood for muscle and didn't change the result that much.](fit_ruvasht_muscle.html) The differences I did observe probably only resulted because I didn't do any posthoc inflation.

## Individual Datasets
* [All null setting.](heart_t.html) RUVASH with a t-likelihood gives really good t-statistics. Calibrated CATE also gives really good t-statistics.
* [80\% null setting.](heart_t_nonnull.html) RUVASH with a t-likelihood again does very well. Calibrated CATE does ok.
* [MAD inflation always improves ASH.](cate_cal.html) Motivated by CATE, I used MAD inflation on the standard errors. This always improved ASH's performance.
* [Variance estimates seem to be OK when simulating under the assumed model.](change_q.html) But RUVASH still estimates a large variance inflation and most methods still underestimate $\pi_0$. RUVASH seems to be over-inflating though.

## Presentations
* [JSM 2016 Draft](jsm_2016.pdf)

## Shiny App

* Visit [here](https://github.com/dcgerard/succotash_sims/tree/master/analysis/mouthwash)

## References
